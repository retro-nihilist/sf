{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "liFg08vGk-Ou"
      },
      "source": [
        "# <center> Практическое задание по теме \"Циклы\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iZQ8hwHMphX3"
      },
      "source": [
        "<img src=https://1gai.ru/uploads/posts/2020-12/1608804716_54441.png width=300 align=\"right\">\n",
        "\n",
        "→ Поздравляем с освоением важных для анализа данных конструкций Python (переменные, структуры данных, условные операторы и циклы)!\n",
        "\n",
        "Настало время промежуточной практики, чтобы закрепить все полученные в предыдущих модулях навыки! Мы будем применять их на реальном проекте. Сегодня мы обратимся к классике: займемся анализом текстов на примере «Войны и мира» Льва Николаевича Толстого!\n",
        "\n",
        "В рамках практического кейса мы сначала вместе познакомимся с исходными данными и произведем некоторые манипуляции над ними. После чего вам предстоит самостоятельно выполнить несколько заданий на тему поиска наиболее значимых слов в тексте с помощью методов статистического анализа текста. \n",
        "\n",
        "> Итак, приступим!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WgQNPhZ4k68p"
      },
      "source": [
        "## <center> Знакомимся с данными"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn4Fncpvk6Va"
      },
      "source": [
        "Текст произведения мы взяли в библиотеке [lib.ru](http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0073.shtml) и провели первоначальную обработку. Поскольку наша цель — обработка слов из этого произведения, мы разбили текст на слова и вывели каждое слово в отдельной строке. Кроме того, в местах, где начинаются главы, мы вывели строку `\"[new chapter]\"`.\n",
        "\n",
        "> Исходный текстовый файл хранится в общем доступе и находится [здесь](https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGItOdWbx4KT"
      },
      "source": [
        "Для начала скачаем текст книги по ссылке."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8cSMJp2anzX",
        "outputId": "84e5f85c-8b7c-4634-a729-b9c195bb7def"
      },
      "outputs": [],
      "source": [
        "# Импортируем библиотеку для выполнения HTTP-запросов в интернет\n",
        "import requests \n",
        "\n",
        "# Читаем текстовый файл по url-ссылке\n",
        "data = requests.get(\"https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt\").text\n",
        "\n",
        "# Предобрабатываем текстовый файл\n",
        "data = data.split('\\n')\n",
        "data.remove('')\n",
        "data = data + ['[new chapter]']\n",
        "\n",
        "# Выводим первые 100 слов из книги\n",
        "print(data[:100])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eeTg-aKgCwye"
      },
      "source": [
        "## <center> Работаем с данными"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bpn_8XFRCo7o"
      },
      "source": [
        "Для начала найдем общее количество слов и количество уникальных слов в тексте"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPNZVk27wkpM",
        "outputId": "4852e43f-b382-4872-d611-e5e2f27d30f3"
      },
      "outputs": [],
      "source": [
        "# Превращаем список в множество, удаляя дублирующиеся слова\n",
        "word_set = set(data)\n",
        "# Удаляем из множества слово, символизирующее раздел между главами\n",
        "word_set.discard('[new chapter]')\n",
        "# Выводим результаты\n",
        "print('Общее количество слов: {}'.format(len(data)))\n",
        "print('Общее количество уникальных слов: {}'.format(len(word_set)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ShCV0QIAqCsG"
      },
      "source": [
        "Давайте напишем программу, которая посчитает частоту каждого слова. Для этого создадим словарь, ключами которого будут являться слова, а значения - количество вхождений этого слова в текст произведения. Заодно подсчитаем количество глав"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzd2YuYZeCf5",
        "outputId": "e0d1e049-a517-4b32-9831-991754b015d5"
      },
      "outputs": [],
      "source": [
        "# Инициализируем пустой словарь\n",
        "word_counts = {}\n",
        "# Инициализируем количество глав\n",
        "count_chapter = 0\n",
        "# Создаем цикл по всем словам из списка слов\n",
        "for word in data:\n",
        "    # Проверяем, что текущее слово - обозначение новой главы\n",
        "    if word == '[new chapter]':\n",
        "        # Если условие выполняется, то увеличиваем количество глав на 1\n",
        "        count_chapter += 1\n",
        "        # Переходим на новую итерацию цикла\n",
        "        continue\n",
        "    # Проверяем, что текущего слова еще нет в словаре слов\n",
        "    if word not in word_counts:\n",
        "        # Если условие выполняется, инициализируем новый ключ 1\n",
        "        word_counts[word] = 1\n",
        "    else:\n",
        "        # В противном случае, увеличиваем количество слов на 1\n",
        "        word_counts[word] += 1\n",
        "\n",
        "# Выводим количество глав\n",
        "print('Количество глав: {}'.format(count_chapter))\n",
        "\n",
        "# Создаем цикл по ключам и их порядковым номерам полученного словаря\n",
        "for i, key in enumerate(word_counts):\n",
        "    # Выводим только первые 10 слов\n",
        "    if i == 10:\n",
        "        break\n",
        "    print(key, word_counts[key])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WRo9-Xr-rFwP"
      },
      "source": [
        "Разделим все слова на главы. Для этого создадим список, в котором будем хранить списки - слова из определенной главы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsdGmz-frFf0",
        "outputId": "60416c2b-9074-4eb6-aa0f-a5b6682ab364"
      },
      "outputs": [],
      "source": [
        "# Инициализируем общий список, в котором будем хранить списки слов в каждой главе\n",
        "chapter_data = []\n",
        "# Инициализируем список слов, в котором будет хранить слова одной главы\n",
        "chapter_words = []\n",
        "\n",
        "# Создаем цикл по всем словам из списка\n",
        "for word in data:\n",
        "    # Проверяем, что текущее слово - обозначение новой главы\n",
        "    if word == '[new chapter]':\n",
        "        # Если условие выполняется, добавляем список со словами из главы в общий список\n",
        "        chapter_data.append(chapter_words)\n",
        "        # Обновляем (перезаписываем) список со словами из текущей главы\n",
        "        chapter_words = []\n",
        "    else:\n",
        "        # В противном случае, добавляем текущее слово в список со словами из главы\n",
        "        chapter_words.append(word)\n",
        "\n",
        "# Проверяем, что у нас получилось столько же списков, сколько глав в произведении\n",
        "print('Вложенный список содержит {} внутренних списка'.format(len(chapter_data)))\n",
        "# Выведем первые 100 слов 0-ой главы\n",
        "print(chapter_data[0][:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TM0OaA3pinfQ",
        "outputId": "00ffd7f0-5970-4d86-eb39-a968448cfc10"
      },
      "outputs": [],
      "source": [
        "chapter_data[15][100]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "saO-dds9h-Mo"
      },
      "source": [
        "Подсчитаем, сколько раз каждое слово встречается в каждой из глав"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oGOvOHxGt3M",
        "outputId": "0bc775a7-1a5f-4912-8e59-7242478fa8aa"
      },
      "outputs": [],
      "source": [
        "# Инициализируем список, в котором будем хранить словари\n",
        "chapter_words_count = []\n",
        "\n",
        "# Создаем цикл по элементам внешнего списка со словами\n",
        "for chapter_words in chapter_data:\n",
        "    # Инициализируем пустой словарь, куда будем добавлять результаты\n",
        "    temp = {}\n",
        "    # Создаем цикл по элементам внутреннего списка\n",
        "    for word in chapter_words:\n",
        "        # Проверяем, что текущего слова еще нет в словаре\n",
        "        if word not in temp:\n",
        "            # Если условие выполняется, добавляем ключ в словарь\n",
        "            temp[word] = 1\n",
        "        else:\n",
        "            # В противном случае, увеличиваем количество влождений слова в главу\n",
        "            temp[word] += 1\n",
        "    # Добавляем получившийся словарь в список\n",
        "    chapter_words_count.append(temp)\n",
        "\n",
        "# Выводим результат\n",
        "print(chapter_words_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytU4Um8vjUqi",
        "outputId": "dc415190-cf84-406f-b252-e26413510413"
      },
      "outputs": [],
      "source": [
        "chapter_words_count[15]['князю']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBIS01d0tYNi",
        "outputId": "c239da04-d68d-4eda-be83-69aa804b89c1"
      },
      "outputs": [],
      "source": [
        "# Создаем цикл по ключам словаря - спискам слов и их порядковым номерам\n",
        "for chapter_number, chapter_dict in enumerate(chapter_words_count):\n",
        "    # Выводим только первые 5 глав\n",
        "    if chapter_number == 5:\n",
        "        break\n",
        "    # Выводим номер главы\n",
        "    print('-' * 40)\n",
        "    print('Chapter: {}'.format(chapter_number))\n",
        "    print('-' * 40)\n",
        "    # Создаем цикл по ключам - словам и их порядковым номерам\n",
        "    for j, word in enumerate(chapter_dict):\n",
        "        # Выводим первые 10 слов из главы\n",
        "        if j == 10:\n",
        "            break\n",
        "        print(word, chapter_dict[word])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hVkFtTfgiMAM"
      },
      "source": [
        "Давайте резюмировать, что мы с вами уже получили:\n",
        "\n",
        "* `word_set` - множество из всех слов, которые есть в книге\n",
        "\n",
        "* `count_chapter` - количество глав в книге (171)\n",
        "\n",
        "* `word_counts` - словарь, ключами которого являются слова, а значениями - количество вхождений этих слов в книгу\n",
        "\n",
        "* `chapter_data` - список из 171 списка, где элементы вложенных списков - все слова из главы. Каждый список соответствует своей главе\n",
        "\n",
        "* `chapter_words_count` - список из 171 словаря, где ключи - слова, а значения - количество слов в главе. Каждый словарь соответствует своей главе\n",
        "\n",
        "Учтите, что эти данные могут пригодиться вам при выполнении дальнейших заданий.\n",
        "\n",
        "> А теперь к заданиями!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Gq7Waelsji"
      },
      "source": [
        "## <center> Задания для самостоятельного решения"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h6gX5LrcKrJw"
      },
      "source": [
        "### Задание 1.\n",
        "\n",
        "Давайте введем понятие частоты употребления отдельного слова в документе (`term frequency`, или `tf`). В нашем случае речь идёт не о документах, а о главах книги (выше мы писали, что в текстовом документе главы разделяются строкой '[new chapter]').\n",
        "\n",
        "Формула для вычисления `term frequency` для слова `word`:\n",
        "$$ tf_{word, chapter} = \\frac {n_{word, chapter}} {n_{chapter}}$$\n",
        "\n",
        "где \n",
        "* ${n_{word, chapter}}$ - сколько раз слово `word` встрачается в главе `chapter`, \n",
        "* $n_{chapter}$ - количество слов в главе `chapter`.\n",
        "\n",
        "\n",
        "Например, слово `\"гостья\"` употребляется в 15-ой главе 10 раз (${n_{word, chapter}}$).(кстати, главы у нас нумеруются с 0). Общее количество слов в тексте 15-ой главы - 1359 ($n_{chapter}$). Тогда:\n",
        "\n",
        "$$ tf_{гостья, 15} = \\frac{10}{1359} \\approx 0.007358$$\n",
        "\n",
        "**Задание:** \n",
        "\n",
        "Напишите программу, которая позволит получать частоту употребления любого заданного слова `target_word` в заданной главе `target_chapter`. \n",
        "\n",
        "**Дополнительное требование:**\n",
        "\n",
        "*Пострайтесь сделать программу максимально обобщенной. То есть желательно рассчитать характеристику `tf` для всех слов из каждой главы, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
        "\n",
        "**Подсказка:**\n",
        "\n",
        "*Для этого вы можете для каждой главы создать словарь, ключами которого являются слова, а значения - частота употребления этого слова в этой главе*\n",
        "\n",
        "**Протестируйте работу программы на нескольких словах и главах.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCs_lzDEPpJZ"
      },
      "outputs": [],
      "source": [
        "target_word = 'полтергейст'\n",
        "target_chapter = 15\n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "dict_len_chapter = {}\n",
        "iter = 0\n",
        "for i in chapter_data: #формируем словарь с главами и количеством слов в главах\n",
        "    len_chapter_words_count = len(i)\n",
        "    dict_len_chapter[iter] = len_chapter_words_count\n",
        "    iter +=1\n",
        "#print(dict_len_chapter)\n",
        "\n",
        "#Формируем словари со значениями частоты по главам\n",
        "tf0=[] #список для словарей со значениями частоты слов из каждой главы\n",
        "iter0 = 0\n",
        "for d in chapter_words_count: #перебираем словари являющиеся главами\n",
        "    #print(d)\n",
        "    tf0_dict = {}\n",
        "    iter1 = 0\n",
        "    for w in d: #перебираем слова в главах и считаем частоту вхождения\n",
        "        #print (d[w])\n",
        "        tf0_dict[w] = d[w]/dict_len_chapter[iter0]\n",
        "        iter1+=1\n",
        "    tf0.append(tf0_dict) #добавляем словарь в список\n",
        "    #print('iter1 ',iter1)\n",
        "    iter0+=1\n",
        "#print ('iter0 ',iter0)\n",
        "#print(tf0[10])\n",
        "\n",
        "#Первый вариан решения ЗАДАЧИ 1 ----------------------------------------------------------Первый вариан решения ЗАДАЧИ 1\n",
        "#Задаётся номер главы и слово, выводится частота появления слова в главе, либо уведомление об отсутствии слова в главе\n",
        "print('Первый вариан решения ЗАДАЧИ 1')\n",
        "print('нумерация глав начинантся с [ 0 ], максимальное значение:', count_chapter)\n",
        "\n",
        "#chapter0 = int(input(\"Введите номер главы:\")) #указываем номер главы\n",
        "chapter0 = target_chapter\n",
        "while chapter0 <=0 or chapter0 >= count_chapter:\n",
        "    print('неверный номер главы, введите корректный номер главы', 'нумерация глав начинантся с 0, максимальное значение:', count_chapter)\n",
        "    chapter0 = int(input(\"номер главы:\")) #указываем номер главы    \n",
        "#target_word = str(input(\"искомое слово :\"))\n",
        "witf=(target_word in tf0[chapter0])\n",
        "while not (target_word in tf0[chapter0]):\n",
        "    #print(target_word != \"nextblok\")\n",
        "    print('слово [', target_word, \"] отсутствует в главе\", chapter0)\n",
        "    print('введите другое слово либо напечатайте nextblok')\n",
        "    target_word = str(input(\"искомое слово:\"))\n",
        "    if target_word == \"nextblok\":break\n",
        "if target_word == \"nextblok\":\n",
        "    print('выбран отказ от поиска слова в Первом варианте решения ЗАДАЧИ 1')\n",
        "else:\n",
        "    print('В главе', chapter0, 'частота использования слова [', target_word, '] составляет', tf0[chapter0][target_word])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XyZ5EhoLRbnG"
      },
      "source": [
        "### Задание 2.\n",
        "\n",
        "Пришло время познакомиться с понятием `document frequency`.\n",
        "\n",
        "`Document frequency` (для удобства сократим до `df`) — это доля документов, в которых встречается искомое слово. \n",
        "\n",
        "Вычисляется по формуле:\n",
        "\n",
        "$$ df_{word} = \\frac{N_{word}}{N} $$, \n",
        "\n",
        "где \n",
        "* $N_{word}$ - число документов (глав) содержащих слово `word`, \n",
        "* $N$ - общее число документов (глав).\n",
        "\n",
        "Объясним на примере: наш текст состоит из 171 главы ($N$), а слово `\"человек\"` встречается в 115 главах. Тогда:\n",
        "\n",
        "$$ df_{человек} = \\frac{115}{171} \\approx 0.6725$$\n",
        "\n",
        "**Задание:** \n",
        "\n",
        "Напишите программу, которая позволит вычислять document frequency для заданного слова `target_word` и выведить результат на экран.\n",
        "\n",
        "**Дополнительное требование:**\n",
        "\n",
        "*Пострайтесь сделать программу максимально обобщенной. То есть желательно рассчитать характеристику `df` для всех уникальных слов из книги, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
        "\n",
        "**Подсказка:**\n",
        "*Для этого вы можете создать словарь, ключами которого являются слова из книги, а значения - доля документов, содержащих эти слова*\n",
        "\n",
        "**Протестируйте работу программы на нескольких словах** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8EN3GZ1YCqB"
      },
      "outputs": [],
      "source": [
        "target_word = 'сударь'\n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "#Второй вариан решения ЗАДАЧИ 1 ----------------------------------------------------------Второй вариан решения ЗАДАЧИ 1\n",
        "#Задаётся слово, выводится частота появления во всех главах и во всей книге\n",
        "#print('Второй вариан решения ЗАДАЧИ 1')\n",
        "tf1_dict_w = {}#Двух Уровневый словарь первый уровень СЛОВО второй Уровень НОМЕР ГЛАВЫ и ЧАСТОТА\n",
        "for w in word_set:#перебираем уникальные слова\n",
        "    tf1_dict = {}\n",
        "    #iter=0\n",
        "    for iter, dict in enumerate(tf0):#проверяем информацию по слову в конкретной главе\n",
        "        if not (w in dict):\n",
        "            tf1_dict[iter] = 0\n",
        "        else:\n",
        "            tf1_dict[iter] = dict[w]\n",
        "        #iter +=1\n",
        "    tf1_dict_w[w] = tf1_dict\n",
        "#print(tf1_dict_w)\n",
        "\n",
        "\"\"\"\n",
        "target_word = str(input(\"искомое слово :\"))\n",
        "periodicity1 = tf1_dict_w[target_word]\n",
        "#print(periodicity1)\n",
        "for chapter in periodicity1:\n",
        "     if periodicity1[chapter] > 0:\n",
        "        print(\"Глава [{}] слово [{}] частота использования [{}]\".format(chapter, target_word, periodicity1[chapter]))\n",
        "     else:continue\n",
        "\"\"\"\n",
        "\n",
        "#Задание #2-----------------------------------------------------------------------------------------------------Задание #2\n",
        "# Вычисляем document frequency (df) для каждого слова из tf1_dict_w\n",
        "df_dict = {}\n",
        "# Перебираем слова из словаря tf1_dict_w\n",
        "for word in tf1_dict_w:\n",
        "    # Считаем количество глав, в которых встречается слово\n",
        "    count_chapters_with_word = sum(1 for chapter in tf1_dict_w[word] if tf1_dict_w[word][chapter] > 0)\n",
        "    # Вычисляем долю глав, содержащих слово\n",
        "    df = count_chapters_with_word / count_chapter\n",
        "    # Добавляем результат в словарь df_dict\n",
        "    df_dict[word] = df\n",
        "\n",
        "print(\"Задание №2. поиск частоты употребления слова [\",target_word, \"] в разрезе глав\")\n",
        "\n",
        "#target_word = str(input(\"искомое слово :\"))\n",
        "try:\n",
        "    print (df_dict[target_word])\n",
        "except:\n",
        "    print('слово [', target_word, \"] отсутствует в тексте\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SS78rqWDYZMo"
      },
      "source": [
        "### Задание 3\n",
        "\n",
        "Пришло время дать разъяснения: для чего мы делали вычисления выше и что нас ждет впереди?\n",
        "\n",
        "> Если какое-то слово часто употребляется в документе, то, вероятно, этот документ что-то рассказывает о предмете/действии, описываемом этим словом. Скажем, если вы читаете книгу, в которой много раз употребляется слово `\"заяц\"`, то, вероятно, эта книга про зайцев.\n",
        "\n",
        "> Однако, если вы возьмёте слово `\"и\"`, то оно будет встречаться почти в каждой книге много раз. \n",
        "\n",
        "Таким образом, если мы хотим найти наиболее значимые слова в книге, мы, с одной стороны, хотим найти наиболее частые слова, а с другой — убрать те, которые не несут важной информации, так как встречаются везде.\n",
        "\n",
        "Такая задача хорошо решается с помощью `tf-idf` — статистической метрики для оценки важности слова в тексте. Другими словами, `tf-idf` — это «контрастность» слова в документе (насколько оно выделяется среди других слов). \n",
        "\n",
        "Формула для вычисления следующая:\n",
        "\n",
        "`tf-idf = term frequency * inverse document frequency`\n",
        "\n",
        "* `tf` — это частотность термина, которая измеряет, насколько часто термин встречается в документе.\n",
        "\n",
        "* `idf` — это обратная документная частотность термина. Она измеряет непосредственно важность термина во всём множестве документов.\n",
        "\n",
        "Чтобы получить `idf`, необходимо поделить 1 на полученную в Задании 2 документную частоту (`df`):\n",
        "\n",
        "$$idf = \\frac{1}{df}$$\n",
        "\n",
        "Мы будем использовать не сырые значения `idf`, а их логарифмы, то есть $tf * log(idf)$. Сейчас мы не будем заострять внимания на том, почему следует использовать именно логарифм — это долгий разговор. Вернемся к нему, когда будем изучать методы машинного обучения для обработки текстов. Подробнее о `tf-idf` вы можете почитать [здесь](https://translated.turbopages.org/proxy_u/en-ru.ru.15518a02-63e76541-6895b80b-74722d776562/https/www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/).\n",
        "\n",
        "В качестве примера измерим `tf-idf` слова `\"анна\"` в главе 4. Слово `\"анна\"` встречается в указанной главе 7 раз, при этом в 4 главе 1060 слов, всего же слово `\"анна\"` упоминается в 32 главах из 171.\n",
        "\n",
        "Таким образом, `tf-idf` данного слова в данной главе будет равно:\n",
        "\n",
        "$$tf\\_idf_{анна, 4} = tf * log(\\frac{1}{df}) = \\frac{7}{1060} * log(\\frac {171}{32}) \\approx 0.011067$$\n",
        "\n",
        "**Примечание:** здесь используется натуральный логарифм по основанию $e$, однако в общем случае основание логарифма не имеет значения, так как характеристика `tf-idf` используется для сравнения контрастности слов между собой\n",
        "\n",
        "**Задание**:\n",
        "\n",
        "Напишите программу, которая позволяет вычислять значение `tf-idf` для заданного слова `target_word` в заданной главе `target_chapter`.\n",
        "\n",
        "**Дополнительное требование:**\n",
        "\n",
        "*Пострайтесь сделать программу максимально оптимальной. То есть желательно рассчитать характеристику `tf-idf` для всех слов из каждой главы книги, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
        "\n",
        "**Протестируйте работу программы на нескольких словах и главах.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MvdwTW_459uh"
      },
      "source": [
        "**Примечание:**\n",
        "\n",
        "Натуральный логарифм можно вычислить с помощью функции [log](https://pythonim.ru/chisla/funktsiya-log-v-python) из встроенного в Python модуля math:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX5UCiwj63FD",
        "outputId": "3f33848f-09e1-4fc0-854b-c590f84150e4"
      },
      "outputs": [],
      "source": [
        "# Импортируем функцию log из модуля math:\n",
        "from math import log\n",
        "print(log(18))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nk_gYbsJ7ayr"
      },
      "source": [
        "**Примечание.** \n",
        "\n",
        "**Модуль (библиотека) в Python** — это любой программный файл, который содержит в себе код, включая функции. В нашем случае math — это встроенный модуль, содержащий функционал для математических вычислений. Подробнее о math вы можете почитать [здесь](https://pythonworld.ru/moduli/modul-math.html). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDB99Uu_2QG-"
      },
      "outputs": [],
      "source": [
        "target_word = 'война'\n",
        "target_chapter = 5\n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "it_idf_dict = {}\n",
        "for word in word_set:\n",
        "    chart =0\n",
        "    temp_dict = {}\n",
        "    for chart in range(count_chapter):\n",
        "        chart_info = tf1_dict_w[word][chart]*log(1/df_dict[word])\n",
        "        temp_dict[chart] = chart_info\n",
        "        chart+=1\n",
        "    it_idf_dict[word] = temp_dict\n",
        "\n",
        "word3 = target_word\n",
        "#word3 = str(input(\"искомое слово :\"))\n",
        "print(\"Определение «контрастности» слова [\",word3,\"] в главе [\", target_chapter,\"]\")\n",
        "print(it_idf_dict[word3][target_chapter])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k_BS6pPmhHdr"
      },
      "source": [
        "### Задание 4.\n",
        "\n",
        "Теперь, когда мы умеем вычислять `tf-idf` для каждого слова в главе, мы можем найти те слова, которые являются самыми «контрастными» для данной главы, то есть они могут являться в своём роде заголовком для главы.\n",
        "\n",
        "Например, для главы 3 наиболее значимыми словами будут:\n",
        "\n",
        "`\"анна\", \"павловна\", \"функе\"`\n",
        "\n",
        "**Задание:**\n",
        "\n",
        "Напишите программу, которая позволяет вывести три слова, имеющие самое высокое значение `tf-idf` в заданной главе `target_chapter` в порядке убывания `tf-idf`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k6BzGFJiIAH"
      },
      "outputs": [],
      "source": [
        "target_chapter = 3\n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "import operator #использовано по подсказке GPT\n",
        "itog_dict = {} #Первыйуровень глава, второй уговень словрь с словом и частотой\n",
        "#print(it_idf_dict)\n",
        "\n",
        "for ch4 in range(count_chapter):\n",
        "    chapter_data_set = set(chapter_data[ch4])\n",
        "    temp_dict0 = {}\n",
        "    for w in chapter_data_set:\n",
        "        temp_dict0[w] = it_idf_dict[w][ch4]\n",
        "    itog_dict[ch4] = temp_dict0\n",
        "        \n",
        "\n",
        "\n",
        "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
        "my_dict.items()  # dict_items([('a', 1), ('b', 2), ('c', 3)])\n",
        "list(my_dict.items())[0]  # ('a', 1)\n",
        "\n",
        "\n",
        "#chart4 = int(input(\"задайте главу для поиска наиболее популярного слова :\"))\n",
        "chart4 = target_chapter\n",
        "#top = int (input(\"Задайте ТОП какого количества популярных слов вывести\"))\n",
        "top = 3\n",
        "try:\n",
        "    temp_list0 = []\n",
        "    temp_dict1 = itog_dict[chart4]\n",
        "    temp_dict1.items()\n",
        "    sorted_temp_dict1 = sorted(temp_dict1.items(), key=lambda x: x[1], reverse=True)\n",
        "   \n",
        "    print(\"Топ [ {} ] контрастных слов в главе [ {} ]\".format(top, chart4)) \n",
        "    i=0\n",
        "    for i in range(top):\n",
        "        try:\n",
        "            print(list(sorted_temp_dict1)[i])\n",
        "        except IndexError:\n",
        "                print('параметр ТОП превышает количество уникальных слов в главе, [', count_chapter, \"]\")\n",
        "except KeyError:    \n",
        "        print('главы [', chart4, \"] не существует\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
